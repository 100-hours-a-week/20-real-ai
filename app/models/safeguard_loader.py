import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from app.models.llm_client import get_chat_response

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
safeguard_model_name= "kakaocorp/kanana-safeguard-prompt-2.1b"

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
safeguard_model = AutoModelForCausalLM.from_pretrained(
    safeguard_model_name,
    torch_dtype=torch.float16,
    device_map="auto"
).eval()

safeguard_tokenizer = AutoTokenizer.from_pretrained(safeguard_model_name)

# SAFE/UNSAFE íŒë³„ í•¨ìˆ˜
def classify_prompt(user_prompt: str) -> str:
    messages = [{"role": "user", "content": user_prompt}]

    # ì±„íŒ… í…œí”Œë¦¿ ì ìš© í›„ í† í°í™”
    input_ids = safeguard_tokenizer.apply_chat_template(messages, tokenize=True, return_tensors="pt").to(safeguard_model.device)
    attention_mask = (input_ids != safeguard_tokenizer.pad_token_id).long()

    # ë‹¤ìŒ í† í° 1ê°œ ìƒì„± 
    with torch.no_grad():
        output_ids = safeguard_model.generate(
            input_ids,
            attention_mask=attention_mask,
            max_new_tokens=1,
            pad_token_id=safeguard_tokenizer.eos_token_id
        )

    # ìƒˆë¡œ ìƒì„±ëœ í† í°ë§Œ ì¶”ì¶œí•´ ë””ì½”ë”©
    gen_idx = input_ids.shape[-1]
    return safeguard_tokenizer.decode(output_ids[0][gen_idx], skip_special_tokens=True).strip() # strip()

#  í†µí•© ì±—ë´‡ ì‘ë‹µ í•¨ìˆ˜
async def secure_chat_response(question: str, request_id: str, user_id: str) -> str:
    safety_token = classify_prompt(question)

    if safety_token == "<SAFE>":
        return await get_chat_response(question, request_id, user_id)
    else:
        return "ì¹´ì¹´ì˜¤í…Œí¬ ë¶€íŠ¸ìº í”„ ê´€ë ¨ ê³µì§€ì‚¬í•­ë§Œ ì§ˆë¬¸í•´ì£¼ì„¸ìš” ğŸ˜ƒ"

