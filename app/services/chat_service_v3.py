from app.core.date_utils import parse_relative_dates
from app.core.vector_loader import load_vectorstore
from app.core.chat_history import get_session_history, chat_history_to_string
from app.models.prompt_template import chatbot_rag_prompt
from app.models.llm_client import get_chat_response_stream
from langsmith.run_helpers import get_current_run_tree
from langsmith import traceable
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from scripts.create_vectorstore import header_splitted_docs, vectorstore

# BM25 ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± í•¨ìˆ˜
def create_bm25_retriever(header_splitted_docs):
    # í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ê¸°
    bm25_retriever = BM25Retriever.from_documents(
        header_splitted_docs,
        )
    bm25_retriever.k = 1
    return bm25_retriever

# FAISS ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± í•¨ìˆ˜
def create_faiss_retriever(vectorstore):  
    # ë²¡í„° ê¸°ë°˜ ê²€ìƒ‰ê¸°
    retriever = vectorstore.as_retriever(
        search_type="similarity_score_threshold",         # ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§
        search_kwargs={"k": 2, "score_threshold": 0.7}
    )
    return retriever

# ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± í•¨ìˆ˜
def create_ensemble_retriever(retrievers, weights=[0.7, 0.3]):
    ensemble_retriever = EnsembleRetriever(
        retrievers=retrievers,
        weights=weights
    )
    return ensemble_retriever

# ìµœì¢… ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
retriever = create_ensemble_retriever(
    [create_bm25_retriever(header_splitted_docs), create_faiss_retriever(vectorstore)],
    weights=[0.7, 0.3]
    )

# ì‚¬ìš©ìë³„ ì„¸ì…˜ íˆìŠ¤í† ë¦¬ì— Q/A message ì €ì¥ 
async def save_chat_history(userId: int, question: str, answer: str):
    history = get_session_history(userId)
    history.add_user_message(question)
    history.add_ai_message(answer)

@traceable(name="Chat Controller V3", inputs={"ì§ˆë¬¸": lambda args, kwargs: args[0]})
async def chat_service_stream(question: str, request_id: str, userId: int):

    # ì§ˆë¬¸ ì „ì²˜ë¦¬ (ìƒëŒ€ ë‚ ì§œ -> ì ˆëŒ€ ë‚ ì§œ)
    parsed_question = parse_relative_dates(question)
        
    # ì‚¬ìš©ì íˆìŠ¤í† ë¦¬ ë¡œë”© ë° ë¬¸ìì—´ ë°˜í™˜
    history = get_session_history(userId)
    history_str = chat_history_to_string(history)
        
     # RAG
    docs = retriever.get_relevant_documents(parsed_question)
    if not docs:
        yield "data: ì¹´ì¹´ì˜¤í…Œí¬ ë¶€íŠ¸ìº í”„ ê´€ë ¨ ê³µì§€ì‚¬í•­ë§Œ ì§ˆë¬¸í•´ì£¼ì„¸ìš” ğŸ˜ƒ\n\n"
        yield "event: end_of_stream\ndata: \n\n"
        return
    context = "\n\n".join([doc.page_content for doc in docs])

    # í”„ë¡¬í”„íŠ¸ ì •ì˜ ë° LLM í˜¸ì¶œ
    prompt = chatbot_rag_prompt.format(history=history_str, context=context, question=parsed_question)
    
    # vLLM ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ 
    agen = get_chat_response_stream(prompt, docs, request_id)

    answer_collector = []
    async for chunk in agen:
        yield f"data: {chunk}\n\n"
        answer_collector.append(chunk)

    full_answer = "".join(answer_collector)

    run = get_current_run_tree()
    if run:
        run.add_outputs({"request_id": request_id,"ë‹µë³€": full_answer})

    # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ì•Œë¦¼ ì´ë²¤íŠ¸
    yield "event: end_of_stream\ndata: \n\n"
    
    # íˆìŠ¤í† ë¦¬ ì €ì¥
    await save_chat_history(userId, parsed_question, full_answer)