from app.core.date_utils import parse_relative_dates
from app.core.vector_loader import load_vectorstore
from app.models.prompt_template import chatbot_rag_prompt
from app.models.llm_client import get_chat_response
from app.core.chat_history import get_session_history, chat_history_to_string

# ìƒìœ„ 2ê°œì˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” retriever êµ¬ì„±
retriever = load_vectorstore().as_retriever(
        search_type="similarity_score_threshold",  
        search_kwargs={"score_threshold": 0.7, "k": 2}
    )

# ì‚¬ìš©ìë³„ ì„¸ì…˜ íˆìŠ¤í† ë¦¬ì— Q/A message ì €ì¥ 
async def save_chat_history(userId: int, question: str, answer: str):
    history = get_session_history(userId)
    history.add_user_message(question)
    history.add_ai_message(answer)

async def chat_service(question: str, request_id: str, userId: int):
    # ì§ˆë¬¸ ì „ì²˜ë¦¬ (ìƒëŒ€ ë‚ ì§œ -> ì ˆëŒ€ ë‚ ì§œ)
    parsed_question = parse_relative_dates(question)
    
    # ì‚¬ìš©ì íˆìŠ¤í† ë¦¬ ë¡œë”© ë° ë¬¸ìì—´ ë°˜í™˜
    history = get_session_history(userId)
    history_str = chat_history_to_string(history)
    
    # RAG
    docs = retriever.get_relevant_documents(parsed_question)
    if not docs:
        return "ì¹´ì¹´ì˜¤í…Œí¬ ë¶€íŠ¸ìº í”„ ê´€ë ¨ ê³µì§€ì‚¬í•­ë§Œ ì§ˆë¬¸í•´ì£¼ì„¸ìš” ğŸ˜ƒ"
    context = "\n\n".join([doc.page_content for doc in docs])

    # í”„ë¡¬í”„íŠ¸ ì •ì˜ ë° LLM í˜¸ì¶œ
    prompt = chatbot_rag_prompt.format(history=history_str, context=context, question=parsed_question)
    result = await get_chat_response(prompt, docs, request_id)
    
    # íˆìŠ¤í† ë¦¬ ì €ì¥
    await save_chat_history(userId, parsed_question, result)